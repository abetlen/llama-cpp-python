# coding=utf-8
# pylint: disable=too-many-lines
# --------------------------------------------------------------------------
# Code generated by Microsoft (R) AutoRest Code Generator (autorest: 3.9.5, generator: @autorest/python@6.4.15)
# Changes may cause incorrect behavior and will be lost if the code is regenerated.
# --------------------------------------------------------------------------

from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union

from .. import _serialization

if TYPE_CHECKING:
    # pylint: disable=unused-import,ungrouped-imports
    from .. import models as _models


class ChatCompletion(_serialization.Model):
    """ChatCompletion.

    All required parameters must be populated in order to send to Azure.

    :ivar id: Id. Required.
    :vartype id: str
    :ivar object: Object. Required. "chat.completion"
    :vartype object: str or ~llama_cpp.client.models.ChatCompletionObject
    :ivar created: Created. Required.
    :vartype created: int
    :ivar model: Model. Required.
    :vartype model: str
    :ivar choices: Choices. Required.
    :vartype choices: list[~llama_cpp.client.models.ChatCompletionChoice]
    :ivar usage: CompletionUsage. Required.
    :vartype usage: ~llama_cpp.client.models.CompletionUsage
    """

    _validation = {
        "id": {"required": True},
        "object": {"required": True},
        "created": {"required": True},
        "model": {"required": True},
        "choices": {"required": True},
        "usage": {"required": True},
    }

    _attribute_map = {
        "id": {"key": "id", "type": "str"},
        "object": {"key": "object", "type": "str"},
        "created": {"key": "created", "type": "int"},
        "model": {"key": "model", "type": "str"},
        "choices": {"key": "choices", "type": "[ChatCompletionChoice]"},
        "usage": {"key": "usage", "type": "CompletionUsage"},
    }

    def __init__(
        self,
        *,
        id: str,  # pylint: disable=redefined-builtin
        object: Union[str, "_models.ChatCompletionObject"],
        created: int,
        model: str,
        choices: List["_models.ChatCompletionChoice"],
        usage: "_models.CompletionUsage",
        **kwargs: Any
    ) -> None:
        """
        :keyword id: Id. Required.
        :paramtype id: str
        :keyword object: Object. Required. "chat.completion"
        :paramtype object: str or ~llama_cpp.client.models.ChatCompletionObject
        :keyword created: Created. Required.
        :paramtype created: int
        :keyword model: Model. Required.
        :paramtype model: str
        :keyword choices: Choices. Required.
        :paramtype choices: list[~llama_cpp.client.models.ChatCompletionChoice]
        :keyword usage: CompletionUsage. Required.
        :paramtype usage: ~llama_cpp.client.models.CompletionUsage
        """
        super().__init__(**kwargs)
        self.id = id
        self.object = object
        self.created = created
        self.model = model
        self.choices = choices
        self.usage = usage


class ChatCompletionChoice(_serialization.Model):
    """ChatCompletionChoice.

    All required parameters must be populated in order to send to Azure.

    :ivar index: Index. Required.
    :vartype index: int
    :ivar message: ChatCompletionMessage. Required.
    :vartype message: ~llama_cpp.client.models.ChatCompletionMessage
    :ivar finish_reason: Finish Reason. Required.
    :vartype finish_reason: str
    """

    _validation = {
        "index": {"required": True},
        "message": {"required": True},
        "finish_reason": {"required": True},
    }

    _attribute_map = {
        "index": {"key": "index", "type": "int"},
        "message": {"key": "message", "type": "ChatCompletionMessage"},
        "finish_reason": {"key": "finish_reason", "type": "str"},
    }

    def __init__(
        self,
        *,
        index: int,
        message: "_models.ChatCompletionMessage",
        finish_reason: str,
        **kwargs: Any
    ) -> None:
        """
        :keyword index: Index. Required.
        :paramtype index: int
        :keyword message: ChatCompletionMessage. Required.
        :paramtype message: ~llama_cpp.client.models.ChatCompletionMessage
        :keyword finish_reason: Finish Reason. Required.
        :paramtype finish_reason: str
        """
        super().__init__(**kwargs)
        self.index = index
        self.message = message
        self.finish_reason = finish_reason


class ChatCompletionMessage(_serialization.Model):
    """ChatCompletionMessage.

    All required parameters must be populated in order to send to Azure.

    :ivar role: Role. Required. Known values are: "assistant", "user", and "system".
    :vartype role: str or ~llama_cpp.client.models.ChatCompletionMessageRole
    :ivar content: Content. Required.
    :vartype content: str
    :ivar user: User.
    :vartype user: str
    """

    _validation = {
        "role": {"required": True},
        "content": {"required": True},
    }

    _attribute_map = {
        "role": {"key": "role", "type": "str"},
        "content": {"key": "content", "type": "str"},
        "user": {"key": "user", "type": "str"},
    }

    def __init__(
        self,
        *,
        role: Union[str, "_models.ChatCompletionMessageRole"],
        content: str,
        user: Optional[str] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword role: Role. Required. Known values are: "assistant", "user", and "system".
        :paramtype role: str or ~llama_cpp.client.models.ChatCompletionMessageRole
        :keyword content: Content. Required.
        :paramtype content: str
        :keyword user: User.
        :paramtype user: str
        """
        super().__init__(**kwargs)
        self.role = role
        self.content = content
        self.user = user


class ChatCompletionRequestMessage(_serialization.Model):
    """ChatCompletionRequestMessage.

    :ivar role: The role of the message. Known values are: "system", "user", and "assistant".
    :vartype role: str or ~llama_cpp.client.models.ChatCompletionRequestMessageRole
    :ivar content: The content of the message.
    :vartype content: str
    """

    _attribute_map = {
        "role": {"key": "role", "type": "str"},
        "content": {"key": "content", "type": "str"},
    }

    def __init__(
        self,
        *,
        role: Union[str, "_models.ChatCompletionRequestMessageRole"] = "user",
        content: str = "",
        **kwargs: Any
    ) -> None:
        """
        :keyword role: The role of the message. Known values are: "system", "user", and "assistant".
        :paramtype role: str or ~llama_cpp.client.models.ChatCompletionRequestMessageRole
        :keyword content: The content of the message.
        :paramtype content: str
        """
        super().__init__(**kwargs)
        self.role = role
        self.content = content


class Completion(_serialization.Model):
    """Completion.

    All required parameters must be populated in order to send to Azure.

    :ivar id: Id. Required.
    :vartype id: str
    :ivar object: Object. Required. "text_completion"
    :vartype object: str or ~llama_cpp.client.models.CompletionObject
    :ivar created: Created. Required.
    :vartype created: int
    :ivar model: Model. Required.
    :vartype model: str
    :ivar choices: Choices. Required.
    :vartype choices: list[~llama_cpp.client.models.CompletionChoice]
    :ivar usage: CompletionUsage. Required.
    :vartype usage: ~llama_cpp.client.models.CompletionUsage
    """

    _validation = {
        "id": {"required": True},
        "object": {"required": True},
        "created": {"required": True},
        "model": {"required": True},
        "choices": {"required": True},
        "usage": {"required": True},
    }

    _attribute_map = {
        "id": {"key": "id", "type": "str"},
        "object": {"key": "object", "type": "str"},
        "created": {"key": "created", "type": "int"},
        "model": {"key": "model", "type": "str"},
        "choices": {"key": "choices", "type": "[CompletionChoice]"},
        "usage": {"key": "usage", "type": "CompletionUsage"},
    }

    def __init__(
        self,
        *,
        id: str,  # pylint: disable=redefined-builtin
        object: Union[str, "_models.CompletionObject"],
        created: int,
        model: str,
        choices: List["_models.CompletionChoice"],
        usage: "_models.CompletionUsage",
        **kwargs: Any
    ) -> None:
        """
        :keyword id: Id. Required.
        :paramtype id: str
        :keyword object: Object. Required. "text_completion"
        :paramtype object: str or ~llama_cpp.client.models.CompletionObject
        :keyword created: Created. Required.
        :paramtype created: int
        :keyword model: Model. Required.
        :paramtype model: str
        :keyword choices: Choices. Required.
        :paramtype choices: list[~llama_cpp.client.models.CompletionChoice]
        :keyword usage: CompletionUsage. Required.
        :paramtype usage: ~llama_cpp.client.models.CompletionUsage
        """
        super().__init__(**kwargs)
        self.id = id
        self.object = object
        self.created = created
        self.model = model
        self.choices = choices
        self.usage = usage


class CompletionChoice(_serialization.Model):
    """CompletionChoice.

    All required parameters must be populated in order to send to Azure.

    :ivar text: Text. Required.
    :vartype text: str
    :ivar index: Index. Required.
    :vartype index: int
    :ivar logprobs: CompletionLogprobs. Required.
    :vartype logprobs: ~llama_cpp.client.models.CompletionLogprobs
    :ivar finish_reason: Finish Reason. Required.
    :vartype finish_reason: str
    """

    _validation = {
        "text": {"required": True},
        "index": {"required": True},
        "logprobs": {"required": True},
        "finish_reason": {"required": True},
    }

    _attribute_map = {
        "text": {"key": "text", "type": "str"},
        "index": {"key": "index", "type": "int"},
        "logprobs": {"key": "logprobs", "type": "CompletionLogprobs"},
        "finish_reason": {"key": "finish_reason", "type": "str"},
    }

    def __init__(
        self,
        *,
        text: str,
        index: int,
        logprobs: "_models.CompletionLogprobs",
        finish_reason: str,
        **kwargs: Any
    ) -> None:
        """
        :keyword text: Text. Required.
        :paramtype text: str
        :keyword index: Index. Required.
        :paramtype index: int
        :keyword logprobs: CompletionLogprobs. Required.
        :paramtype logprobs: ~llama_cpp.client.models.CompletionLogprobs
        :keyword finish_reason: Finish Reason. Required.
        :paramtype finish_reason: str
        """
        super().__init__(**kwargs)
        self.text = text
        self.index = index
        self.logprobs = logprobs
        self.finish_reason = finish_reason


class CompletionLogprobs(_serialization.Model):
    """CompletionLogprobs.

    All required parameters must be populated in order to send to Azure.

    :ivar text_offset: Text Offset. Required.
    :vartype text_offset: list[int]
    :ivar token_logprobs: Token Logprobs. Required.
    :vartype token_logprobs: list[float]
    :ivar tokens: Tokens. Required.
    :vartype tokens: list[str]
    :ivar top_logprobs: Top Logprobs. Required.
    :vartype top_logprobs: list[dict[str, float]]
    """

    _validation = {
        "text_offset": {"required": True},
        "token_logprobs": {"required": True},
        "tokens": {"required": True},
        "top_logprobs": {"required": True},
    }

    _attribute_map = {
        "text_offset": {"key": "text_offset", "type": "[int]"},
        "token_logprobs": {"key": "token_logprobs", "type": "[float]"},
        "tokens": {"key": "tokens", "type": "[str]"},
        "top_logprobs": {"key": "top_logprobs", "type": "[{float}]"},
    }

    def __init__(
        self,
        *,
        text_offset: List[int],
        token_logprobs: List[float],
        tokens: List[str],
        top_logprobs: List[Dict[str, float]],
        **kwargs: Any
    ) -> None:
        """
        :keyword text_offset: Text Offset. Required.
        :paramtype text_offset: list[int]
        :keyword token_logprobs: Token Logprobs. Required.
        :paramtype token_logprobs: list[float]
        :keyword tokens: Tokens. Required.
        :paramtype tokens: list[str]
        :keyword top_logprobs: Top Logprobs. Required.
        :paramtype top_logprobs: list[dict[str, float]]
        """
        super().__init__(**kwargs)
        self.text_offset = text_offset
        self.token_logprobs = token_logprobs
        self.tokens = tokens
        self.top_logprobs = top_logprobs


class CompletionUsage(_serialization.Model):
    """CompletionUsage.

    All required parameters must be populated in order to send to Azure.

    :ivar prompt_tokens: Prompt Tokens. Required.
    :vartype prompt_tokens: int
    :ivar completion_tokens: Completion Tokens. Required.
    :vartype completion_tokens: int
    :ivar total_tokens: Total Tokens. Required.
    :vartype total_tokens: int
    """

    _validation = {
        "prompt_tokens": {"required": True},
        "completion_tokens": {"required": True},
        "total_tokens": {"required": True},
    }

    _attribute_map = {
        "prompt_tokens": {"key": "prompt_tokens", "type": "int"},
        "completion_tokens": {"key": "completion_tokens", "type": "int"},
        "total_tokens": {"key": "total_tokens", "type": "int"},
    }

    def __init__(
        self,
        *,
        prompt_tokens: int,
        completion_tokens: int,
        total_tokens: int,
        **kwargs: Any
    ) -> None:
        """
        :keyword prompt_tokens: Prompt Tokens. Required.
        :paramtype prompt_tokens: int
        :keyword completion_tokens: Completion Tokens. Required.
        :paramtype completion_tokens: int
        :keyword total_tokens: Total Tokens. Required.
        :paramtype total_tokens: int
        """
        super().__init__(**kwargs)
        self.prompt_tokens = prompt_tokens
        self.completion_tokens = completion_tokens
        self.total_tokens = total_tokens


class CreateChatCompletionRequest(
    _serialization.Model
):  # pylint: disable=too-many-instance-attributes
    """CreateChatCompletionRequest.

    :ivar messages: A list of messages to generate completions for.
    :vartype messages: list[~llama_cpp.client.models.ChatCompletionRequestMessage]
    :ivar max_tokens: The maximum number of tokens to generate.
    :vartype max_tokens: int
    :ivar temperature: Adjust the randomness of the generated text.

     Temperature is a hyperparameter that controls the randomness of the generated text. It affects
     the probability distribution of the model's output tokens. A higher temperature (e.g., 1.5)
     makes the output more random and creative, while a lower temperature (e.g., 0.5) makes the
     output more focused, deterministic, and conservative. The default value is 0.8, which provides
     a balance between randomness and determinism. At the extreme, a temperature of 0 will always
     pick the most likely next token, leading to identical outputs in each run.
    :vartype temperature: float
    :ivar top_p: Limit the next token selection to a subset of tokens with a cumulative probability
     above a threshold P.

     Top-p sampling, also known as nucleus sampling, is another text generation method that selects
     the next token from a subset of tokens that together have a cumulative probability of at least
     p. This method provides a balance between diversity and quality by considering both the
     probabilities of tokens and the number of tokens to sample from. A higher value for top_p
     (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more
     focused and conservative text.
    :vartype top_p: float
    :ivar stop: A list of tokens at which to stop generation. If None, no stop tokens are used.
    :vartype stop: list[str]
    :ivar stream: Whether to stream the results as they are generated. Useful for chatbots.
    :vartype stream: bool
    :ivar presence_penalty: Positive values penalize new tokens based on whether they appear in the
     text so far, increasing the model's likelihood to talk about new topics.
    :vartype presence_penalty: float
    :ivar frequency_penalty: Positive values penalize new tokens based on their existing frequency
     in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
    :vartype frequency_penalty: float
    :ivar model: The model to use for generating completions.
    :vartype model: str
    :ivar n: Parameter "n" is not supported, but accepted / documented here to maintain
     compatibility with OpenAI's API. It may be supported in the future. Check back here for
     updates.
    :vartype n: int
    :ivar logit_bias: Parameter "logit_bias" is not supported, but accepted / documented here to
     maintain compatibility with OpenAI's API. It may be supported in the future. Check back here
     for updates.
    :vartype logit_bias: dict[str, float]
    :ivar user: Parameter "user" is not supported, but accepted / documented here to maintain
     compatibility with OpenAI's API. It may be supported in the future. Check back here for
     updates.
    :vartype user: str
    :ivar top_k: Limit the next token selection to the K most probable tokens.

     Top-k sampling is a text generation method that selects the next token only from the top k
     most likely tokens predicted by the model. It helps reduce the risk of generating
     low-probability or nonsensical tokens, but it may also limit the diversity of the output. A
     higher value for top_k (e.g., 100) will consider more tokens and lead to more diverse text,
     while a lower value (e.g., 10) will focus on the most probable tokens and generate more
     conservative text.
    :vartype top_k: int
    :ivar repeat_penalty: A penalty applied to each token that is already generated. This helps
     prevent the model from repeating itself.

     Repeat penalty is a hyperparameter used to penalize the repetition of token sequences during
     text generation. It helps prevent the model from generating repetitive or monotonous text. A
     higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g.,
     0.9) will be more lenient.
    :vartype repeat_penalty: float
    """

    _validation = {
        "max_tokens": {"maximum": 2048, "minimum": 1},
        "temperature": {"maximum": 2, "minimum": 0},
        "top_p": {"maximum": 1, "minimum": 0},
        "presence_penalty": {"maximum": 2, "minimum": -2},
        "frequency_penalty": {"maximum": 2, "minimum": -2},
        "top_k": {"minimum": 0},
        "repeat_penalty": {"minimum": 0},
    }

    _attribute_map = {
        "messages": {"key": "messages", "type": "[ChatCompletionRequestMessage]"},
        "max_tokens": {"key": "max_tokens", "type": "int"},
        "temperature": {"key": "temperature", "type": "float"},
        "top_p": {"key": "top_p", "type": "float"},
        "stop": {"key": "stop", "type": "[str]"},
        "stream": {"key": "stream", "type": "bool"},
        "presence_penalty": {"key": "presence_penalty", "type": "float"},
        "frequency_penalty": {"key": "frequency_penalty", "type": "float"},
        "model": {"key": "model", "type": "str"},
        "n": {"key": "n", "type": "int"},
        "logit_bias": {"key": "logit_bias", "type": "{float}"},
        "user": {"key": "user", "type": "str"},
        "top_k": {"key": "top_k", "type": "int"},
        "repeat_penalty": {"key": "repeat_penalty", "type": "float"},
    }

    def __init__(
        self,
        *,
        messages: List["_models.ChatCompletionRequestMessage"] = [],
        max_tokens: int = 16,
        temperature: float = 0.8,
        top_p: float = 0.95,
        stop: Optional[List[str]] = None,
        stream: bool = False,
        presence_penalty: float = 0,
        frequency_penalty: float = 0,
        model: Optional[str] = None,
        n: int = 1,
        logit_bias: Optional[Dict[str, float]] = None,
        user: Optional[str] = None,
        top_k: int = 40,
        repeat_penalty: float = 1.1,
        **kwargs: Any
    ) -> None:
        """
        :keyword messages: A list of messages to generate completions for.
        :paramtype messages: list[~llama_cpp.client.models.ChatCompletionRequestMessage]
        :keyword max_tokens: The maximum number of tokens to generate.
        :paramtype max_tokens: int
        :keyword temperature: Adjust the randomness of the generated text.

         Temperature is a hyperparameter that controls the randomness of the generated text. It affects
         the probability distribution of the model's output tokens. A higher temperature (e.g., 1.5)
         makes the output more random and creative, while a lower temperature (e.g., 0.5) makes the
         output more focused, deterministic, and conservative. The default value is 0.8, which provides
         a balance between randomness and determinism. At the extreme, a temperature of 0 will always
         pick the most likely next token, leading to identical outputs in each run.
        :paramtype temperature: float
        :keyword top_p: Limit the next token selection to a subset of tokens with a cumulative
         probability above a threshold P.

         Top-p sampling, also known as nucleus sampling, is another text generation method that selects
         the next token from a subset of tokens that together have a cumulative probability of at least
         p. This method provides a balance between diversity and quality by considering both the
         probabilities of tokens and the number of tokens to sample from. A higher value for top_p
         (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more
         focused and conservative text.
        :paramtype top_p: float
        :keyword stop: A list of tokens at which to stop generation. If None, no stop tokens are used.
        :paramtype stop: list[str]
        :keyword stream: Whether to stream the results as they are generated. Useful for chatbots.
        :paramtype stream: bool
        :keyword presence_penalty: Positive values penalize new tokens based on whether they appear in
         the text so far, increasing the model's likelihood to talk about new topics.
        :paramtype presence_penalty: float
        :keyword frequency_penalty: Positive values penalize new tokens based on their existing
         frequency in the text so far, decreasing the model's likelihood to repeat the same line
         verbatim.
        :paramtype frequency_penalty: float
        :keyword model: The model to use for generating completions.
        :paramtype model: str
        :keyword n: Parameter "n" is not supported, but accepted / documented here to maintain
         compatibility with OpenAI's API. It may be supported in the future. Check back here for
         updates.
        :paramtype n: int
        :keyword logit_bias: Parameter "logit_bias" is not supported, but accepted / documented here to
         maintain compatibility with OpenAI's API. It may be supported in the future. Check back here
         for updates.
        :paramtype logit_bias: dict[str, float]
        :keyword user: Parameter "user" is not supported, but accepted / documented here to maintain
         compatibility with OpenAI's API. It may be supported in the future. Check back here for
         updates.
        :paramtype user: str
        :keyword top_k: Limit the next token selection to the K most probable tokens.

         Top-k sampling is a text generation method that selects the next token only from the top k
         most likely tokens predicted by the model. It helps reduce the risk of generating
         low-probability or nonsensical tokens, but it may also limit the diversity of the output. A
         higher value for top_k (e.g., 100) will consider more tokens and lead to more diverse text,
         while a lower value (e.g., 10) will focus on the most probable tokens and generate more
         conservative text.
        :paramtype top_k: int
        :keyword repeat_penalty: A penalty applied to each token that is already generated. This helps
         prevent the model from repeating itself.

         Repeat penalty is a hyperparameter used to penalize the repetition of token sequences during
         text generation. It helps prevent the model from generating repetitive or monotonous text. A
         higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g.,
         0.9) will be more lenient.
        :paramtype repeat_penalty: float
        """
        super().__init__(**kwargs)
        self.messages = messages
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.top_p = top_p
        self.stop = stop
        self.stream = stream
        self.presence_penalty = presence_penalty
        self.frequency_penalty = frequency_penalty
        self.model = model
        self.n = n
        self.logit_bias = logit_bias
        self.user = user
        self.top_k = top_k
        self.repeat_penalty = repeat_penalty


class CreateCompletionRequest(
    _serialization.Model
):  # pylint: disable=too-many-instance-attributes
    """CreateCompletionRequest.

    :ivar prompt: The prompt to generate completions for.
    :vartype prompt: str
    :ivar suffix: A suffix to append to the generated text. If None, no suffix is appended. Useful
     for chatbots.
    :vartype suffix: str
    :ivar max_tokens: The maximum number of tokens to generate.
    :vartype max_tokens: int
    :ivar temperature: Adjust the randomness of the generated text.

     Temperature is a hyperparameter that controls the randomness of the generated text. It affects
     the probability distribution of the model's output tokens. A higher temperature (e.g., 1.5)
     makes the output more random and creative, while a lower temperature (e.g., 0.5) makes the
     output more focused, deterministic, and conservative. The default value is 0.8, which provides
     a balance between randomness and determinism. At the extreme, a temperature of 0 will always
     pick the most likely next token, leading to identical outputs in each run.
    :vartype temperature: float
    :ivar top_p: Limit the next token selection to a subset of tokens with a cumulative probability
     above a threshold P.

     Top-p sampling, also known as nucleus sampling, is another text generation method that selects
     the next token from a subset of tokens that together have a cumulative probability of at least
     p. This method provides a balance between diversity and quality by considering both the
     probabilities of tokens and the number of tokens to sample from. A higher value for top_p
     (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more
     focused and conservative text.
    :vartype top_p: float
    :ivar echo: Whether to echo the prompt in the generated text. Useful for chatbots.
    :vartype echo: bool
    :ivar stop: A list of tokens at which to stop generation. If None, no stop tokens are used.
    :vartype stop: ~llama_cpp.client.models.CreateCompletionRequestStop
    :ivar stream: Whether to stream the results as they are generated. Useful for chatbots.
    :vartype stream: bool
    :ivar logprobs: Parameter "logprobs" is not supported, but accepted / documented here to
     maintain compatibility with OpenAI's API. It may be supported in the future. Check back here
     for updates.
    :vartype logprobs: int
    :ivar presence_penalty: Positive values penalize new tokens based on whether they appear in the
     text so far, increasing the model's likelihood to talk about new topics.
    :vartype presence_penalty: float
    :ivar frequency_penalty: Positive values penalize new tokens based on their existing frequency
     in the text so far, decreasing the model's likelihood to repeat the same line verbatim.
    :vartype frequency_penalty: float
    :ivar model: The model to use for generating completions.
    :vartype model: str
    :ivar n: Parameter "n" is not supported, but accepted / documented here to maintain
     compatibility with OpenAI's API. It may be supported in the future. Check back here for
     updates.
    :vartype n: int
    :ivar best_of: Parameter "best_of" is not supported, but accepted / documented here to maintain
     compatibility with OpenAI's API. It may be supported in the future. Check back here for
     updates.
    :vartype best_of: int
    :ivar logit_bias: Parameter "logit_bias" is not supported, but accepted / documented here to
     maintain compatibility with OpenAI's API. It may be supported in the future. Check back here
     for updates.
    :vartype logit_bias: dict[str, float]
    :ivar user: Parameter "user" is not supported, but accepted / documented here to maintain
     compatibility with OpenAI's API. It may be supported in the future. Check back here for
     updates.
    :vartype user: str
    :ivar top_k: Limit the next token selection to the K most probable tokens.

     Top-k sampling is a text generation method that selects the next token only from the top k
     most likely tokens predicted by the model. It helps reduce the risk of generating
     low-probability or nonsensical tokens, but it may also limit the diversity of the output. A
     higher value for top_k (e.g., 100) will consider more tokens and lead to more diverse text,
     while a lower value (e.g., 10) will focus on the most probable tokens and generate more
     conservative text.
    :vartype top_k: int
    :ivar repeat_penalty: A penalty applied to each token that is already generated. This helps
     prevent the model from repeating itself.

     Repeat penalty is a hyperparameter used to penalize the repetition of token sequences during
     text generation. It helps prevent the model from generating repetitive or monotonous text. A
     higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g.,
     0.9) will be more lenient.
    :vartype repeat_penalty: float
    """

    _validation = {
        "max_tokens": {"maximum": 2048, "minimum": 1},
        "temperature": {"maximum": 2, "minimum": 0},
        "top_p": {"maximum": 1, "minimum": 0},
        "presence_penalty": {"maximum": 2, "minimum": -2},
        "frequency_penalty": {"maximum": 2, "minimum": -2},
        "top_k": {"minimum": 0},
        "repeat_penalty": {"minimum": 0},
    }

    _attribute_map = {
        "prompt": {"key": "prompt", "type": "str"},
        "suffix": {"key": "suffix", "type": "str"},
        "max_tokens": {"key": "max_tokens", "type": "int"},
        "temperature": {"key": "temperature", "type": "float"},
        "top_p": {"key": "top_p", "type": "float"},
        "echo": {"key": "echo", "type": "bool"},
        "stop": {"key": "stop", "type": "CreateCompletionRequestStop"},
        "stream": {"key": "stream", "type": "bool"},
        "logprobs": {"key": "logprobs", "type": "int"},
        "presence_penalty": {"key": "presence_penalty", "type": "float"},
        "frequency_penalty": {"key": "frequency_penalty", "type": "float"},
        "model": {"key": "model", "type": "str"},
        "n": {"key": "n", "type": "int"},
        "best_of": {"key": "best_of", "type": "int"},
        "logit_bias": {"key": "logit_bias", "type": "{float}"},
        "user": {"key": "user", "type": "str"},
        "top_k": {"key": "top_k", "type": "int"},
        "repeat_penalty": {"key": "repeat_penalty", "type": "float"},
    }

    def __init__(
        self,
        *,
        prompt: str = "",
        suffix: Optional[str] = None,
        max_tokens: int = 16,
        temperature: float = 0.8,
        top_p: float = 0.95,
        echo: bool = False,
        stop: Optional["_models.CreateCompletionRequestStop"] = None,
        stream: bool = False,
        logprobs: Optional[int] = None,
        presence_penalty: float = 0,
        frequency_penalty: float = 0,
        model: Optional[str] = None,
        n: int = 1,
        best_of: int = 1,
        logit_bias: Optional[Dict[str, float]] = None,
        user: Optional[str] = None,
        top_k: int = 40,
        repeat_penalty: float = 1.1,
        **kwargs: Any
    ) -> None:
        """
        :keyword prompt: The prompt to generate completions for.
        :paramtype prompt: str
        :keyword suffix: A suffix to append to the generated text. If None, no suffix is appended.
         Useful for chatbots.
        :paramtype suffix: str
        :keyword max_tokens: The maximum number of tokens to generate.
        :paramtype max_tokens: int
        :keyword temperature: Adjust the randomness of the generated text.

         Temperature is a hyperparameter that controls the randomness of the generated text. It affects
         the probability distribution of the model's output tokens. A higher temperature (e.g., 1.5)
         makes the output more random and creative, while a lower temperature (e.g., 0.5) makes the
         output more focused, deterministic, and conservative. The default value is 0.8, which provides
         a balance between randomness and determinism. At the extreme, a temperature of 0 will always
         pick the most likely next token, leading to identical outputs in each run.
        :paramtype temperature: float
        :keyword top_p: Limit the next token selection to a subset of tokens with a cumulative
         probability above a threshold P.

         Top-p sampling, also known as nucleus sampling, is another text generation method that selects
         the next token from a subset of tokens that together have a cumulative probability of at least
         p. This method provides a balance between diversity and quality by considering both the
         probabilities of tokens and the number of tokens to sample from. A higher value for top_p
         (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more
         focused and conservative text.
        :paramtype top_p: float
        :keyword echo: Whether to echo the prompt in the generated text. Useful for chatbots.
        :paramtype echo: bool
        :keyword stop: A list of tokens at which to stop generation. If None, no stop tokens are used.
        :paramtype stop: ~llama_cpp.client.models.CreateCompletionRequestStop
        :keyword stream: Whether to stream the results as they are generated. Useful for chatbots.
        :paramtype stream: bool
        :keyword logprobs: Parameter "logprobs" is not supported, but accepted / documented here to
         maintain compatibility with OpenAI's API. It may be supported in the future. Check back here
         for updates.
        :paramtype logprobs: int
        :keyword presence_penalty: Positive values penalize new tokens based on whether they appear in
         the text so far, increasing the model's likelihood to talk about new topics.
        :paramtype presence_penalty: float
        :keyword frequency_penalty: Positive values penalize new tokens based on their existing
         frequency in the text so far, decreasing the model's likelihood to repeat the same line
         verbatim.
        :paramtype frequency_penalty: float
        :keyword model: The model to use for generating completions.
        :paramtype model: str
        :keyword n: Parameter "n" is not supported, but accepted / documented here to maintain
         compatibility with OpenAI's API. It may be supported in the future. Check back here for
         updates.
        :paramtype n: int
        :keyword best_of: Parameter "best_of" is not supported, but accepted / documented here to
         maintain compatibility with OpenAI's API. It may be supported in the future. Check back here
         for updates.
        :paramtype best_of: int
        :keyword logit_bias: Parameter "logit_bias" is not supported, but accepted / documented here to
         maintain compatibility with OpenAI's API. It may be supported in the future. Check back here
         for updates.
        :paramtype logit_bias: dict[str, float]
        :keyword user: Parameter "user" is not supported, but accepted / documented here to maintain
         compatibility with OpenAI's API. It may be supported in the future. Check back here for
         updates.
        :paramtype user: str
        :keyword top_k: Limit the next token selection to the K most probable tokens.

         Top-k sampling is a text generation method that selects the next token only from the top k
         most likely tokens predicted by the model. It helps reduce the risk of generating
         low-probability or nonsensical tokens, but it may also limit the diversity of the output. A
         higher value for top_k (e.g., 100) will consider more tokens and lead to more diverse text,
         while a lower value (e.g., 10) will focus on the most probable tokens and generate more
         conservative text.
        :paramtype top_k: int
        :keyword repeat_penalty: A penalty applied to each token that is already generated. This helps
         prevent the model from repeating itself.

         Repeat penalty is a hyperparameter used to penalize the repetition of token sequences during
         text generation. It helps prevent the model from generating repetitive or monotonous text. A
         higher value (e.g., 1.5) will penalize repetitions more strongly, while a lower value (e.g.,
         0.9) will be more lenient.
        :paramtype repeat_penalty: float
        """
        super().__init__(**kwargs)
        self.prompt = prompt
        self.suffix = suffix
        self.max_tokens = max_tokens
        self.temperature = temperature
        self.top_p = top_p
        self.echo = echo
        self.stop = stop
        self.stream = stream
        self.logprobs = logprobs
        self.presence_penalty = presence_penalty
        self.frequency_penalty = frequency_penalty
        self.model = model
        self.n = n
        self.best_of = best_of
        self.logit_bias = logit_bias
        self.user = user
        self.top_k = top_k
        self.repeat_penalty = repeat_penalty


class CreateCompletionRequestStop(_serialization.Model):
    """A list of tokens at which to stop generation. If None, no stop tokens are used."""

    _attribute_map = {}

    def __init__(self, **kwargs: Any) -> None:
        """ """
        super().__init__(**kwargs)


class CreateEmbeddingRequest(_serialization.Model):
    """CreateEmbeddingRequest.

    All required parameters must be populated in order to send to Azure.

    :ivar model: The model to use for generating completions.
    :vartype model: str
    :ivar input: The input to embed. Required.
    :vartype input: str
    :ivar user: User.
    :vartype user: str
    """

    _validation = {
        "input": {"required": True},
    }

    _attribute_map = {
        "model": {"key": "model", "type": "str"},
        "input": {"key": "input", "type": "str"},
        "user": {"key": "user", "type": "str"},
    }

    def __init__(
        self,
        *,
        input: str,
        model: Optional[str] = None,
        user: Optional[str] = None,
        **kwargs: Any
    ) -> None:
        """
        :keyword model: The model to use for generating completions.
        :paramtype model: str
        :keyword input: The input to embed. Required.
        :paramtype input: str
        :keyword user: User.
        :paramtype user: str
        """
        super().__init__(**kwargs)
        self.model = model
        self.input = input
        self.user = user


class Embedding(_serialization.Model):
    """Embedding.

    All required parameters must be populated in order to send to Azure.

    :ivar object: Object. Required. "list"
    :vartype object: str or ~llama_cpp.client.models.EmbeddingObject
    :ivar model: Model. Required.
    :vartype model: str
    :ivar data: Data. Required.
    :vartype data: list[~llama_cpp.client.models.EmbeddingData]
    :ivar usage: EmbeddingUsage. Required.
    :vartype usage: ~llama_cpp.client.models.EmbeddingUsage
    """

    _validation = {
        "object": {"required": True},
        "model": {"required": True},
        "data": {"required": True},
        "usage": {"required": True},
    }

    _attribute_map = {
        "object": {"key": "object", "type": "str"},
        "model": {"key": "model", "type": "str"},
        "data": {"key": "data", "type": "[EmbeddingData]"},
        "usage": {"key": "usage", "type": "EmbeddingUsage"},
    }

    def __init__(
        self,
        *,
        object: Union[str, "_models.EmbeddingObject"],
        model: str,
        data: List["_models.EmbeddingData"],
        usage: "_models.EmbeddingUsage",
        **kwargs: Any
    ) -> None:
        """
        :keyword object: Object. Required. "list"
        :paramtype object: str or ~llama_cpp.client.models.EmbeddingObject
        :keyword model: Model. Required.
        :paramtype model: str
        :keyword data: Data. Required.
        :paramtype data: list[~llama_cpp.client.models.EmbeddingData]
        :keyword usage: EmbeddingUsage. Required.
        :paramtype usage: ~llama_cpp.client.models.EmbeddingUsage
        """
        super().__init__(**kwargs)
        self.object = object
        self.model = model
        self.data = data
        self.usage = usage


class EmbeddingData(_serialization.Model):
    """EmbeddingData.

    All required parameters must be populated in order to send to Azure.

    :ivar index: Index. Required.
    :vartype index: int
    :ivar object: Object. Required.
    :vartype object: str
    :ivar embedding: Embedding. Required.
    :vartype embedding: list[float]
    """

    _validation = {
        "index": {"required": True},
        "object": {"required": True},
        "embedding": {"required": True},
    }

    _attribute_map = {
        "index": {"key": "index", "type": "int"},
        "object": {"key": "object", "type": "str"},
        "embedding": {"key": "embedding", "type": "[float]"},
    }

    def __init__(
        self, *, index: int, object: str, embedding: List[float], **kwargs: Any
    ) -> None:
        """
        :keyword index: Index. Required.
        :paramtype index: int
        :keyword object: Object. Required.
        :paramtype object: str
        :keyword embedding: Embedding. Required.
        :paramtype embedding: list[float]
        """
        super().__init__(**kwargs)
        self.index = index
        self.object = object
        self.embedding = embedding


class EmbeddingUsage(_serialization.Model):
    """EmbeddingUsage.

    All required parameters must be populated in order to send to Azure.

    :ivar prompt_tokens: Prompt Tokens. Required.
    :vartype prompt_tokens: int
    :ivar total_tokens: Total Tokens. Required.
    :vartype total_tokens: int
    """

    _validation = {
        "prompt_tokens": {"required": True},
        "total_tokens": {"required": True},
    }

    _attribute_map = {
        "prompt_tokens": {"key": "prompt_tokens", "type": "int"},
        "total_tokens": {"key": "total_tokens", "type": "int"},
    }

    def __init__(self, *, prompt_tokens: int, total_tokens: int, **kwargs: Any) -> None:
        """
        :keyword prompt_tokens: Prompt Tokens. Required.
        :paramtype prompt_tokens: int
        :keyword total_tokens: Total Tokens. Required.
        :paramtype total_tokens: int
        """
        super().__init__(**kwargs)
        self.prompt_tokens = prompt_tokens
        self.total_tokens = total_tokens


class HTTPValidationError(_serialization.Model):
    """HTTPValidationError.

    :ivar detail: Detail.
    :vartype detail: list[~llama_cpp.client.models.ValidationError]
    """

    _attribute_map = {
        "detail": {"key": "detail", "type": "[ValidationError]"},
    }

    def __init__(
        self, *, detail: Optional[List["_models.ValidationError"]] = None, **kwargs: Any
    ) -> None:
        """
        :keyword detail: Detail.
        :paramtype detail: list[~llama_cpp.client.models.ValidationError]
        """
        super().__init__(**kwargs)
        self.detail = detail


class ModelData(_serialization.Model):
    """ModelData.

    All required parameters must be populated in order to send to Azure.

    :ivar id: Id. Required.
    :vartype id: str
    :ivar object: Object. Required. "model"
    :vartype object: str or ~llama_cpp.client.models.ModelDataObject
    :ivar owned_by: Owned By. Required.
    :vartype owned_by: str
    :ivar permissions: Permissions. Required.
    :vartype permissions: list[str]
    """

    _validation = {
        "id": {"required": True},
        "object": {"required": True},
        "owned_by": {"required": True},
        "permissions": {"required": True},
    }

    _attribute_map = {
        "id": {"key": "id", "type": "str"},
        "object": {"key": "object", "type": "str"},
        "owned_by": {"key": "owned_by", "type": "str"},
        "permissions": {"key": "permissions", "type": "[str]"},
    }

    def __init__(
        self,
        *,
        id: str,  # pylint: disable=redefined-builtin
        object: Union[str, "_models.ModelDataObject"],
        owned_by: str,
        permissions: List[str],
        **kwargs: Any
    ) -> None:
        """
        :keyword id: Id. Required.
        :paramtype id: str
        :keyword object: Object. Required. "model"
        :paramtype object: str or ~llama_cpp.client.models.ModelDataObject
        :keyword owned_by: Owned By. Required.
        :paramtype owned_by: str
        :keyword permissions: Permissions. Required.
        :paramtype permissions: list[str]
        """
        super().__init__(**kwargs)
        self.id = id
        self.object = object
        self.owned_by = owned_by
        self.permissions = permissions


class ModelList(_serialization.Model):
    """ModelList.

    All required parameters must be populated in order to send to Azure.

    :ivar object: Object. Required. "list"
    :vartype object: str or ~llama_cpp.client.models.ModelListObject
    :ivar data: Data. Required.
    :vartype data: list[~llama_cpp.client.models.ModelData]
    """

    _validation = {
        "object": {"required": True},
        "data": {"required": True},
    }

    _attribute_map = {
        "object": {"key": "object", "type": "str"},
        "data": {"key": "data", "type": "[ModelData]"},
    }

    def __init__(
        self,
        *,
        object: Union[str, "_models.ModelListObject"],
        data: List["_models.ModelData"],
        **kwargs: Any
    ) -> None:
        """
        :keyword object: Object. Required. "list"
        :paramtype object: str or ~llama_cpp.client.models.ModelListObject
        :keyword data: Data. Required.
        :paramtype data: list[~llama_cpp.client.models.ModelData]
        """
        super().__init__(**kwargs)
        self.object = object
        self.data = data


class ValidationError(_serialization.Model):
    """ValidationError.

    All required parameters must be populated in order to send to Azure.

    :ivar loc: Location. Required.
    :vartype loc: list[~llama_cpp.client.models.ValidationErrorLocItem]
    :ivar msg: Message. Required.
    :vartype msg: str
    :ivar type: Error Type. Required.
    :vartype type: str
    """

    _validation = {
        "loc": {"required": True},
        "msg": {"required": True},
        "type": {"required": True},
    }

    _attribute_map = {
        "loc": {"key": "loc", "type": "[ValidationErrorLocItem]"},
        "msg": {"key": "msg", "type": "str"},
        "type": {"key": "type", "type": "str"},
    }

    def __init__(
        self,
        *,
        loc: List["_models.ValidationErrorLocItem"],
        msg: str,
        type: str,
        **kwargs: Any
    ) -> None:
        """
        :keyword loc: Location. Required.
        :paramtype loc: list[~llama_cpp.client.models.ValidationErrorLocItem]
        :keyword msg: Message. Required.
        :paramtype msg: str
        :keyword type: Error Type. Required.
        :paramtype type: str
        """
        super().__init__(**kwargs)
        self.loc = loc
        self.msg = msg
        self.type = type


class ValidationErrorLocItem(_serialization.Model):
    """ValidationErrorLocItem."""

    _attribute_map = {}

    def __init__(self, **kwargs: Any) -> None:
        """ """
        super().__init__(**kwargs)
